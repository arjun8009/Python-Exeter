{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Neural Networks \n",
    "## ECM3412/ECMM409 - Nature Inspired Computation\n",
    "\n",
    "In this tutorial, we will learn how to build a multi-layer perceptron (MLP) to solve the logic XOR gate problem. In addition to that, we will look into how to use Python `scikit-learn` to build a MLP classfier for a real world dataset - [The wine dataset](https://archive.ics.uci.edu/ml/datasets/wine).\n",
    "\n",
    "To conduct this tutorial, please make sure you have `numpy`, `pandas`, `matplotlib` and `scikit-learn` installed on your local machine. Alternatively, you can use [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) to run your code on Google cloud. (You will need a Goolgle account to do so.)\n",
    "\n",
    "**Intended learning outcomes:**\n",
    "- To familise yourself with the learning algorithm for MLP, in particular, the feedforwad and backpropagation phases involved in training a neural network. \n",
    "- To gain hands-on experience on building neural networks model using Python `scikit-learn`.\n",
    "- To understand how to evaluate model performance.\n",
    "- To understand how to tune parameters to achieve better performance.\n",
    "- To understand why standardisation may help improving the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP for the XOR problem\n",
    "\n",
    "\n",
    "###  The structure of a neural network with one hidden layer\n",
    "\n",
    "![nn_pic.png](nn_pic.png)\n",
    "\n",
    "Please note that the bias terms are introduced in the above diagram. Bias is just like an intercept added in a linear equation. It is an additional parameter in the neural network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Moreover, bias value allows you to shift the activation function to either right or left\n",
    "\n",
    "The XOR logic gate returns **True (1)** when the two Boolean inputs are different, otherwise it returns **False (0)**.\n",
    "Here is the simple training dataset.\n",
    "\n",
    "**x1**|**x2**|**y**\n",
    ":-----:|:-----:|:-----:\n",
    "0|0|0\n",
    "0|1|1\n",
    "1|0|1\n",
    "1|1|0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For array operations\n",
    "import matplotlib.pyplot as plt # For plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training data as a numpy array\n",
    "# Please note that the first column is bias\n",
    "X = np.array([[1, 0, 0],\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [1, 1, 1]])\n",
    "\n",
    "# The labels for the training data.\n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_i_units = 3 # Number of Input units (bias included)\n",
    "num_h_units = 2 # Number of Hidden units\n",
    "num_o_units = 1 # Number of Output units\n",
    "\n",
    "# The learning rate for Gradient Descent.\n",
    "learning_rate = 0.15\n",
    "# error\n",
    "costs = []   # a list to record the cost of the NN after each Gradient Descent iteration.\n",
    "\n",
    "# number of epochs\n",
    "epochs = 10000\n",
    "\n",
    "# Number of training examples\n",
    "m = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "These are the parameters that the neural network needs to learn in order to make accurate predictions.\n",
    "\n",
    "For the connections being made from the input layer to the hidden layer, the weights and biases are arranged in the following order: **each column contains the weights for each hidden unit**. Then, the shape of these set of weights is: *(number of input units $\\times$ number of hidden units)*. \n",
    "\n",
    "So, the overall shape of the weights and biases are:\n",
    "\n",
    "**Weights1 (Connection from input to hidden layers)**: num_i_units $\\times$ num_h_units\n",
    "\n",
    "**Weights2 (Connection from hidden to output layers)**: num_h_units $\\times$ num_o_units\n",
    "\n",
    "### Initialising the Weights and Biases\n",
    "\n",
    "The weights here are going to be generated using a [Normal Distribution(Gaussian Distribution)](http://mathworld.wolfram.com/NormalDistribution.html). They will also be seeded so that the outcome always comes out the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results. Bear in mind how different random states will affect the algorithm's convergence.\n",
    "np.random.seed(3412) \n",
    "\n",
    "W1 = np.random.randn(num_i_units, num_h_units) # \n",
    "W2 = np.random.randn(num_h_units+1,1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Sigmoid (Logistic) function as the activation function. The sigmoid function is a non-linear function that maps any input to a value between 0 and 1.\n",
    "![](sigmoid-curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function: sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid for backpropagation\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "In the phase of forward propagation, the inputs are fed into the network to compute the prediction.\n",
    "In this implementation, the forward function accepts feature matrix with each row representing a feature vector for a single sample. Also, the predict boolean, if set to true, only returns the output. Otherwise, it returns the outputs of all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a forward function to calculate the predictions \n",
    "def forward(x, W1, W2, predict=False):\n",
    "\n",
    "    a1 = np.matmul(x, W1)  # pre-activation for the hidden layer (4x3)x(3x2)-->(4x2)\n",
    "    z1 = sigmoid(a1)  # output of the hidden layer (4x2)-->(4x2)\n",
    "    \n",
    "    # create and add bias\n",
    "    bias = np.ones((len(z1), 1))  # bias term for hidden (4x1)\n",
    "    z1 = np.concatenate((bias, z1), axis=1)  # condatenate bias terms for hidden layer\n",
    "    a2 = np.matmul(z1, W2)  # pre-activation for the output neuron\n",
    "    z2 = sigmoid(a2)  # output\n",
    "\n",
    "    if predict: \n",
    "        return z2\n",
    "    return a1, z1, a2, z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The process of propagating the error in the output layer, backwards through the NN to calculate the error in each layer. Intuition: It's like forward propagation, but backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop function\n",
    "def backprop(a2, z0, z1, z2, y):\n",
    "    delta2 = z2-y  # output - target\n",
    "    Delta2 = np.matmul(z1.T, delta2)\n",
    "    \n",
    "    delta1 = (delta2.dot(W2[1:,:].T))*sigmoid_deriv(a1)\n",
    "    Delta1 = np.matmul(z0.T, delta1)\n",
    "    \n",
    "    return delta2, Delta1, Delta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "This is the training function which contains the operations in both forward propagation and backpropagation phases.\n",
    "\n",
    "The gradients(errors) of the weights and biases are used to update the corresponding weights and biases by multiplying them with the negative of the learning rate and scaling it by dividing it by the number of training examples.\n",
    "\n",
    "While iterating over all the training examples, the cost is also being calculated simultaneously for each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z1, a2, z2 = forward(X, W1, W2)\n",
    "\n",
    "    # Back propagation\n",
    "    delta2, Delta1, Delta2 = backprop(a2, X, z1, z2, y)\n",
    "\n",
    "    W1 = W1 - learning_rate*(1/m)*Delta1\n",
    "    W2 = W2 - learning_rate*(1/m)*Delta2\n",
    "\n",
    "    # Add costs to list for plotting\n",
    "    c = np.mean(np.abs(delta2))\n",
    "    costs.append(c)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration: {i}; Error {c}\")\n",
    "\n",
    "# Training complete\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the trained weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3 = forward(X, W1, W2, True)\n",
    "print(f\"Percentages:\\n {z3}\\n\")\n",
    "print(f\"Predictions:\\n {np.round(z3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "Plot the error signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assigning the axes to the different elements.\n",
    "plt.plot(range(epochs), costs)\n",
    "\n",
    "# Labelling the x axis as the iterations axis.\n",
    "plt.xlabel(\"Iterations\")\n",
    "\n",
    "# Labelling the y axis as the cost axis.\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# Showing the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1. How does the learning rate parameter affect the convergence of the learning algorithm? \n",
    "2. How does the number of epochs affect the convergence of the learning algorithm?\n",
    "3. How does the structure of the neural network affect the convergence of the learning algorithm? (Tips: try to change the number of units on the hidden layers)? How about the number of hidden layers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Wine Dataset\n",
    "### Dataset Information:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    ">These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "The attributes are (dontated by Riccardo Leardi, riclea '@' anchem.unige.it )\n",
    " 0. **Wine class**\n",
    " 1. Alcohol\n",
    " 2. Malic acid\n",
    " 3. Ash \n",
    " 4. Alcalinity of ash\n",
    " 5. Magnesium \n",
    " 6. Total phenols \n",
    " 7. Flavanoids\n",
    " 8. Nonflavanoid phenols\n",
    " 9. Proanthocyanins\n",
    " 10. Color intensity\n",
    " 11. Hue\n",
    " 12. OD280/OD315 of diluted wines\n",
    " 13. Proline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\",header=None)\n",
    "print(wine.shape)\n",
    "wine.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine[np.arange(1,14)]\n",
    "y = wine[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task is to split the data into the training and testing sets, using the sklearn `train_test_split` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3412 # set random state\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train our MLP Neural Network with **50 units** in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=50, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us measure the performance of our NN using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(clf.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us use cross-validation to have a more reliable estimation of the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cv = MLPClassifier(hidden_layer_sizes=50, random_state=seed)\n",
    "scores = cross_val_score(clf_cv, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: mean: {scores.mean()} standard deviation: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Now, your task is to use cross-validation to find the best number of hidden units in the network. \n",
    "You will have to do the following:\n",
    " * Iterate over a range of number of hidden units [20,30,40,50,60,70] and measure the performance of the classifier using K-Fold cross validation (K=5) for each of these configurations. \n",
    " \n",
    " What configuration produced the best perfomance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will rescale the data before the training. \n",
    "Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "[Scikit-learn standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "[Why do we need scaling?](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35#:~:text=Feature%20scaling%20is%20essential%20for,that%20calculate%20distances%20between%20data.&text=Since%20the%20range%20of%20values,not%20work%20correctly%20without%20normalization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    " * Repeat the experiments above on the normalized datasets.\n",
    " * How much the peformance increased compared with the unormalized dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
