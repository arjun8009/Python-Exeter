{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods: AdaBoost\n",
    "\n",
    "Understand and visualise the boosting principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test utilities\n",
    "def is_approximately_equal(test,target,eps=1e-2):\n",
    "    return np.mean(np.fabs(np.array(test) - np.array(target)))<eps\n",
    "\n",
    "def assert_test_equality(test, target):\n",
    "    assert is_approximately_equal(test, target), 'Expected:\\n %s \\nbut got:\\n %s'%(target, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `plot` to visualise the decision surface of a predictor (a single weak predictor or a strong one) and mark the importance of instances if the information is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, y, preds=None, instance_weights=None, predict_func=None):\n",
    "    cmap='bwr'\n",
    "    if instance_weights is None:\n",
    "        instance_weights = np.ones(X.shape[0])/X.shape[0]\n",
    "    res = 20\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, res),np.linspace(y_min, y_max, res))\n",
    "    if predict_func is None:\n",
    "        Z = np.ones(xx.shape)*.5\n",
    "    else:\n",
    "        Z = predict_func(np.hstack([xx.reshape(-1,1), yy.reshape(-1,1)]))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "    if np.mean(Z) != -1:\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.contourf(xx, yy, Z, levels=20, cmap=cmap, alpha=.5)\n",
    "        if preds is None:\n",
    "            colors = 'w'\n",
    "        else:\n",
    "            colors = ['w' if status else 'k' for status in (y == preds)]\n",
    "        scaled_instance_weights = instance_weights*X.shape[0] * 20\n",
    "        plt.scatter(X[:,0],X[:,1], c=y, s=scaled_instance_weights, edgecolors=colors, cmap=cmap)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Make a function `error(targets, preds, weights=None)` that computes the fraction of incorrect predictions with respect to the input targets. If the input array `weights` is avaialble then each error is weighted by the corresponding weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(targets, preds, weights=None):\n",
    "    if(weights is not None):\n",
    "        weighted_error = [weights[i] for i in range(len(preds)) if preds[i]!=targets[i]]\n",
    "        error  = sum(weighted_error)\n",
    "        return error\n",
    "    else:\n",
    "        unweighted_error = [1 for i in range(len(preds)) if preds[i]!=targets[i]]\n",
    "        error  = sum(unweighted_error)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "test_target=np.array([1,-1])\n",
    "test_preds=np.array([1,1])\n",
    "test_weights=np.array([.1,.9])\n",
    "e = error(test_target, test_preds, test_weights)\n",
    "assert e==0.9, 'Calling error on the test data should yield 0.9 rather than %.2f'%e\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Write the function `Wm, alpha, error = update_weights(Wm, preds, targets)` that implements the adaboost strategy to compute the updated weights `Wm`, the importance score `alpha` and the error value `error`.\n",
    "\n",
    "See the lecture notes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(Wm, preds, targets):\n",
    "    errors = error(targets,preds,Wm)\n",
    "    numerator = 1-errors\n",
    "    classifier_error = 0.5*np.log(numerator/errors)\n",
    "    updated_weights=  []\n",
    "    indicator = [1 if preds[i]!=targets[i] else 0 for i in range(len(targets))]\n",
    "    for i in range(len(Wm)):\n",
    "        exponent = np.round(np.exp(classifier_error*indicator[i]),2)\n",
    "        updated_weights.append(np.round(Wm[i]*exponent,2))\n",
    "    updated_weights = np.array([np.round(i/sum(updated_weights),2) for i in updated_weights])\n",
    "    return updated_weights,classifier_error,errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "test_Wm = np.array([0.1,0.2,0.5,0.2])\n",
    "test_preds = np.array([1,-1,1,1])\n",
    "test_targets = np.array([-1,1,1,1])\n",
    "test_Wm, test_alpha, test_error = update_weights(test_Wm, test_preds, test_targets)\n",
    "target_Wm = np.array([0.13, 0.26, 0.43, 0.17])\n",
    "assert_test_equality(test_Wm, target_Wm)\n",
    "target_alpha = 0.423\n",
    "assert_test_equality(test_alpha, target_alpha)\n",
    "target_error = 0.3\n",
    "assert_test_equality(test_error, target_error)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "a) Make the function `Wm = initialise_weights(X)` to initialise the instance weights for the AdaBoost algorithm.\n",
    "\n",
    "b) Study the following code for the function `train_adaboost(X_train, y_train, n_iter, max_depth=1, display=False)`. This will require your functions `initialise_weights` and `update_weights` to be correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_weights(X):\n",
    "    N = len(X)\n",
    "    weights = [1/N for i in range(N)]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "test_X = np.array([[1,1],[-1,-1],[0.5,0.5],[-0.5,-0.5]])\n",
    "test_Wm = initialise_weights(test_X)\n",
    "target_Wm = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "assert_test_equality(test_Wm, target_Wm)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_adaboost(X_train, y_train, n_iter, max_depth=1, display=False):\n",
    "    models = [] \n",
    "    errors = []\n",
    "    alphas = []\n",
    "    weights = []\n",
    "    Wm = initialise_weights(X_train)\n",
    "    for m in range(n_iter):\n",
    "        \"\"\"fint and predict using a weak classifier: a decision tree of depth=max_depth \"\"\"\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth).fit(X_train,y_train,sample_weight=Wm)\n",
    "        preds = clf.predict(X_train)\n",
    "        \n",
    "        \"\"\"update instances' weights based on predictive error\"\"\"\n",
    "        Wm, alpha, e = update_weights(Wm, preds, y_train)\n",
    "        \n",
    "        models.append((alpha,clf))       #Store the trained model (clf) and its weight (alpha)\n",
    "\n",
    "        if display and m<10: #display only the first 10 updates\n",
    "            plot(X_train, y_train, preds=None, instance_weights=Wm, predict_func=clf.predict)\n",
    "\n",
    "        \"\"\"Log info for downstream analysis\"\"\"\n",
    "        errors.append(e)          \n",
    "        alphas.append(alpha)      \n",
    "        weights.append(Wm.copy())\n",
    "    return models, errors, alphas, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "\n",
    "test_X_train = np.array([[1,1],[-1,-1],[0.5,0.5],[-0.5,-0.5]])\n",
    "test_y_train = np.array([1,-1,-1,1])\n",
    "test_models, test_errors, test_alphas, test_weights = train_adaboost(test_X_train, test_y_train, n_iter=1, max_depth=1, display=False)\n",
    "\n",
    "assert len(test_models) == 1 and len(test_models[0]) == 2, 'Expected one model-alpha pair only'\n",
    "assert_test_equality(test_errors, [0.25])\n",
    "assert_test_equality(test_alphas, [0.5493])\n",
    "target_weights = np.array([0.21, 0.21, 0.36 , 0.21])\n",
    "assert_test_equality(test_weights, target_weights)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Make the function `X,y = make_data()` that loads the iris dataset from scikit [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) and transforms the data in the following way:\n",
    "1. the data matrix will preserve only feature with id 0 and id 2, i.e. the data matrix will contain two dimensinal instances\n",
    "2. the targets will be recoded as follows: \n",
    "- class 1 -> class 1\n",
    "- class 0 -> class -1\n",
    "- class 2 -> class -1\n",
    "\n",
    "When plotting the data via `plot(X,y)` you should obtain something like:\n",
    "\n",
    "<img src='plot.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "X,y = make_data()\n",
    "assert len(set(y)) == 2, 'Expecting 2 classes only'\n",
    "assert X.shape[1] == 2, 'Expecting two dimensional data matrix'\n",
    "assert np.sum(y == -1) == 100 and np.sum(y == 1) == 50, 'Expecting 50 positive and 100 negative instances'\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Before running the following cells, think about what you expect the result to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X,y = make_data()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "\n",
    "models, errors, alphas, weights = train_adaboost(X_train, y_train, n_iter=100, max_depth=1, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Before running the following cells, think about what you expect the result to be.\n",
    "\n",
    "Why do you get the behavior shown in the plots for the error and the alpha coefficient as the number of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you expect here?\n",
    "\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(alphas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Before running the following cells, think about what you expect the result to be.\n",
    "\n",
    "Which are the instances that will receive most weight? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_difficult_instances(X_train, y_train, W, top_k=20):\n",
    "    idx = np.argsort(-W)[:top_k]\n",
    "    Wb = np.ones(W.shape) \n",
    "    Wb[idx] = 10\n",
    "    Wb = Wb / np.sum(Wb)\n",
    "    plot(X_train, y_train, instance_weights=Wb)\n",
    "\n",
    "W = weights[-1]\n",
    "plot_difficult_instances(X_train, y_train, W, top_k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "a) Make the function `predict_adaboost(X_test, models)` that returns the predicted class array for a data matrix containing test instances, using the AdaBoost algorithm and the already fit models + alpha avaliable in `models`.\n",
    "\n",
    "b) Make a function `score_adaboost(X_test, models)` that return the average predicted class (i.e. a real value between -1 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(X_test, models):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def score_adaboost(X_test, models):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. \n",
    "### BEGIN TESTS\n",
    "\n",
    "test_X_train = np.array([[1,1],[-1,-1],[0.5,0.5],[-0.5,-0.5]])\n",
    "test_y_train = np.array([1,-1,-1,1])\n",
    "test_models, test_errors, test_alphas, test_weights = train_adaboost(test_X_train, test_y_train, n_iter=1, max_depth=1, display=False)\n",
    "\n",
    "eps=0.25\n",
    "test_X_test = np.array([[1+eps,1],[-1+eps,-1],[0.5+eps,0.5],[-0.5+eps,-0.5]])\n",
    "test_preds = predict_adaboost(test_X_test, test_models)\n",
    "target_preds = np.array([ 1., -1.,  1.,  1.])\n",
    "assert_test_equality(test_preds, target_preds)\n",
    "\n",
    "test_scores = score_adaboost(test_X_test, test_models)\n",
    "target_scores = np.array([ 0.54, -0.54,  0.54,  0.54])\n",
    "assert_test_equality(test_scores, target_scores)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Try the following code. \n",
    "\n",
    "```python\n",
    "e = error(y_test, predict_adaboost(X_test, models))\n",
    "print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "plot(X_test, predict_adaboost(X_test, models), y_test)\n",
    "```\n",
    "\n",
    "You should obtain a result similar to:\n",
    "\n",
    "<img src='plot2.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e = error(y_test, predict_adaboost(X_test, models))\n",
    "print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "plot(X_test, predict_adaboost(X_test, models), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material\n",
    "\n",
    "In the following cells you can find some code that allows you to play with an artificial classification problem and study the behavior of the AdaBoost algorithm. \n",
    "\n",
    "Try to change the max_depth parameter and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the code to make a classifier object following the scikit style\n",
    "\n",
    "class MyAdaBoostClassfier(object):\n",
    "    def __init__(self, n_iter, max_depth=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.max_depth=max_depth\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.models, errors, alphas, weights = train_adaboost(X_train, y_train, n_iter=self.n_iter, max_depth=self.max_depth, display=False)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return predict_adaboost(X_test, self.models)\n",
    "        \n",
    "    def decision_function(self, X_test):\n",
    "        return score_adaboost(X_test, self.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a classification data set\n",
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.0, class_sep=1.0)\n",
    "y[y==0]=-1\n",
    "plot(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the decision surface of the AdaBoostClassifier as implemented by scikit\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=100).fit(X_train, y_train)\n",
    "e = error(y_test, clf.predict(X_test))\n",
    "print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "plot(X_test, y_test, preds=clf.predict(X_test), predict_func=clf.decision_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare it to your implementation\n",
    "clf = MyAdaBoostClassfier(n_iter=100, max_depth=1).fit(X_train, y_train)\n",
    "e = error(y_test, clf.predict(X_test))\n",
    "print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "plot(X_test, y_test, preds=clf.predict(X_test), predict_func=clf.decision_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the behavior of the AdaBoost classifier as the number of weak learners increases  \n",
    "for n_iter in [3,31,301]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "    clf = MyAdaBoostClassfier(n_iter=n_iter, max_depth=1).fit(X_train, y_train)\n",
    "    e = error(y_test, clf.predict(X_test))\n",
    "    print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "    plot(X_test, y_test, preds=clf.predict(X_test), predict_func=clf.decision_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the behavior of the AdaBoost classifier as the number of weak learners increases \n",
    "# but using more powerful base classifiers, i.e. decision trees with greater depth\n",
    "for n_iter in [3,31,301]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "    clf = MyAdaBoostClassfier(n_iter=n_iter, max_depth=3).fit(X_train, y_train)\n",
    "    e = error(y_test, clf.predict(X_test))\n",
    "    print('Acc:%.3f  Err:%.3f'%(1-e,e))\n",
    "    plot(X_test, y_test, preds=clf.predict(X_test), predict_func=clf.decision_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b98f472bb8ba48098397e3b897b5be76f7bf0e62d98845cdb0e8066dc5677259"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
